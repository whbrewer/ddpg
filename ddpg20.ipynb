{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control - 20 agents\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher20.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_seed = 2. Average Score: 0.0982<br>\n",
    "random_seed = 3. Average Score: 0.0793"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make separate agents for every arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating agents\n",
      "\n",
      "Episode 1\n",
      "iteration: 1000/1000\tavg score: 9.77e-01\tbest agent: 15\tscore: 2.09e+00 \tdeque: 9.77500e-01\n",
      "checkpointing agent 15 with score 2.09e+00\n",
      "\n",
      "Episode 2\n",
      "iteration: 1000/1000\tavg score: 8.37e-01\tbest agent: 16\tscore: 1.72e+00 \tdeque: 9.07500e-01\n",
      "Episode 3\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 4\tscore: 2.50e+00 \tdeque: 9.83667e-01\n",
      "checkpointing agent 4 with score 2.50e+00\n",
      "\n",
      "Episode 4\n",
      "iteration: 1000/1000\tavg score: 1.12e+00\tbest agent: 17\tscore: 2.70e+00 \tdeque: 1.01762e+00\n",
      "checkpointing agent 17 with score 2.70e+00\n",
      "\n",
      "Episode 5\n",
      "iteration: 1000/1000\tavg score: 8.53e-01\tbest agent: 16\tscore: 1.83e+00 \tdeque: 9.84700e-01\n",
      "Episode 6\n",
      "iteration: 1000/1000\tavg score: 9.46e-01\tbest agent: 6\tscore: 1.93e+00 \tdeque: 9.78250e-01\n",
      "Episode 7\n",
      "iteration: 1000/1000\tavg score: 1.00e+00\tbest agent: 11\tscore: 2.73e+00 \tdeque: 9.82000e-01\n",
      "checkpointing agent 11 with score 2.73e+00\n",
      "\n",
      "Episode 8\n",
      "iteration: 1000/1000\tavg score: 7.99e-01\tbest agent: 17\tscore: 1.88e+00 \tdeque: 9.59187e-01\n",
      "Episode 9\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 17\tscore: 2.54e+00 \tdeque: 9.73000e-01\n",
      "Episode 10\n",
      "iteration: 1000/1000\tavg score: 9.87e-01\tbest agent: 16\tscore: 2.16e+00 \tdeque: 9.74450e-01\n",
      "Episode 11\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 0\tscore: 3.08e+00 \tdeque: 9.88455e-01\n",
      "checkpointing agent 0 with score 3.08e+00\n",
      "\n",
      "Episode 12\n",
      "iteration: 1000/1000\tavg score: 8.16e-01\tbest agent: 12\tscore: 1.50e+00 \tdeque: 9.74083e-01\n",
      "Episode 13\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 10\tscore: 2.19e+00 \tdeque: 9.81077e-01\n",
      "Episode 14\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 15\tscore: 2.06e+00 \tdeque: 9.87714e-01\n",
      "Episode 15\n",
      "iteration: 1000/1000\tavg score: 8.07e-01\tbest agent: 16\tscore: 2.05e+00 \tdeque: 9.75667e-01\n",
      "Episode 16\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 3\tscore: 2.03e+00 \tdeque: 9.78750e-01\n",
      "Episode 17\n",
      "iteration: 1000/1000\tavg score: 8.19e-01\tbest agent: 4\tscore: 2.04e+00 \tdeque: 9.69353e-01\n",
      "Episode 18\n",
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 19\tscore: 1.85e+00 \tdeque: 9.79694e-01\n",
      "Episode 19\n",
      "iteration: 1000/1000\tavg score: 9.71e-01\tbest agent: 11\tscore: 2.04e+00 \tdeque: 9.79237e-01\n",
      "Episode 20\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 19\tscore: 2.14e+00 \tdeque: 9.86700e-01\n",
      "Episode 21\n",
      "iteration: 1000/1000\tavg score: 1.30e+00\tbest agent: 2\tscore: 2.80e+00 \tdeque: 1.00150e+00\n",
      "Episode 22\n",
      "iteration: 1000/1000\tavg score: 8.59e-01\tbest agent: 9\tscore: 1.96e+00 \tdeque: 9.95023e-01\n",
      "Episode 23\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 7\tscore: 2.07e+00 \tdeque: 9.96283e-01\n",
      "Episode 24\n",
      "iteration: 1000/1000\tavg score: 9.47e-01\tbest agent: 0\tscore: 1.98e+00 \tdeque: 9.94250e-01\n",
      "Episode 25\n",
      "iteration: 1000/1000\tavg score: 8.12e-01\tbest agent: 13\tscore: 2.20e+00 \tdeque: 9.86960e-01\n",
      "Episode 26\n",
      "iteration: 1000/1000\tavg score: 9.87e-01\tbest agent: 0\tscore: 2.02e+00 \tdeque: 9.86981e-01\n",
      "Episode 27\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 10\tscore: 2.42e+00 \tdeque: 9.92389e-01\n",
      "Episode 28\n",
      "iteration: 1000/1000\tavg score: 9.07e-01\tbest agent: 11\tscore: 1.58e+00 \tdeque: 9.89339e-01\n",
      "Episode 29\n",
      "iteration: 1000/1000\tavg score: 9.18e-01\tbest agent: 4\tscore: 1.98e+00 \tdeque: 9.86879e-01\n",
      "Episode 30\n",
      "iteration: 1000/1000\tavg score: 8.64e-01\tbest agent: 3\tscore: 1.72e+00 \tdeque: 9.82783e-01\n",
      "Episode 31\n",
      "iteration: 1000/1000\tavg score: 9.89e-01\tbest agent: 4\tscore: 1.85e+00 \tdeque: 9.83000e-01\n",
      "Episode 32\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 0\tscore: 3.95e+00 \tdeque: 9.84547e-01\n",
      "checkpointing agent 0 with score 3.95e+00\n",
      "\n",
      "Episode 33\n",
      "iteration: 1000/1000\tavg score: 9.24e-01\tbest agent: 11\tscore: 2.20e+00 \tdeque: 9.82712e-01\n",
      "Episode 34\n",
      "iteration: 1000/1000\tavg score: 9.66e-01\tbest agent: 8\tscore: 1.77e+00 \tdeque: 9.82221e-01\n",
      "Episode 35\n",
      "iteration: 1000/1000\tavg score: 8.74e-01\tbest agent: 10\tscore: 1.74e+00 \tdeque: 9.79143e-01\n",
      "Episode 36\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 9\tscore: 2.29e+00 \tdeque: 9.83750e-01\n",
      "Episode 37\n",
      "iteration: 1000/1000\tavg score: 8.87e-01\tbest agent: 7\tscore: 2.51e+00 \tdeque: 9.81149e-01\n",
      "Episode 38\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 15\tscore: 2.73e+00 \tdeque: 9.83171e-01\n",
      "Episode 39\n",
      "iteration: 1000/1000\tavg score: 8.48e-01\tbest agent: 15\tscore: 1.54e+00 \tdeque: 9.79705e-01\n",
      "Episode 40\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 9\tscore: 2.72e+00 \tdeque: 9.80825e-01\n",
      "Episode 41\n",
      "iteration: 1000/1000\tavg score: 9.99e-01\tbest agent: 6\tscore: 2.34e+00 \tdeque: 9.81268e-01\n",
      "Episode 42\n",
      "iteration: 1000/1000\tavg score: 9.79e-01\tbest agent: 9\tscore: 2.37e+00 \tdeque: 9.81214e-01\n",
      "Episode 43\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 2\tscore: 3.59e+00 \tdeque: 9.82767e-01\n",
      "Episode 44\n",
      "iteration: 1000/1000\tavg score: 9.77e-01\tbest agent: 8\tscore: 2.60e+00 \tdeque: 9.82636e-01\n",
      "Episode 45\n",
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 16\tscore: 2.19e+00 \tdeque: 9.86644e-01\n",
      "Episode 46\n",
      "iteration: 1000/1000\tavg score: 9.73e-01\tbest agent: 14\tscore: 1.80e+00 \tdeque: 9.86359e-01\n",
      "Episode 47\n",
      "iteration: 1000/1000\tavg score: 9.39e-01\tbest agent: 8\tscore: 2.38e+00 \tdeque: 9.85351e-01\n",
      "Episode 48\n",
      "iteration: 1000/1000\tavg score: 1.27e+00\tbest agent: 15\tscore: 3.06e+00 \tdeque: 9.91281e-01\n",
      "Episode 49\n",
      "iteration: 1000/1000\tavg score: 9.14e-01\tbest agent: 2\tscore: 2.36e+00 \tdeque: 9.89704e-01\n",
      "Episode 50\n",
      "iteration: 1000/1000\tavg score: 8.96e-01\tbest agent: 4\tscore: 1.61e+00 \tdeque: 9.87840e-01\n",
      "Episode 51\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 14\tscore: 2.71e+00 \tdeque: 9.89265e-01\n",
      "Episode 52\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 3\tscore: 3.48e+00 \tdeque: 9.92000e-01\n",
      "Episode 53\n",
      "iteration: 1000/1000\tavg score: 1.12e+00\tbest agent: 7\tscore: 2.90e+00 \tdeque: 9.94396e-01\n",
      "Episode 54\n",
      "iteration: 1000/1000\tavg score: 9.42e-01\tbest agent: 14\tscore: 1.85e+00 \tdeque: 9.93426e-01\n",
      "Episode 55\n",
      "iteration: 1000/1000\tavg score: 8.30e-01\tbest agent: 10\tscore: 1.33e+00 \tdeque: 9.90455e-01\n",
      "Episode 56\n",
      "iteration: 1000/1000\tavg score: 8.84e-01\tbest agent: 10\tscore: 1.79e+00 \tdeque: 9.88554e-01\n",
      "Episode 57\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 14\tscore: 2.18e+00 \tdeque: 9.90237e-01\n",
      "Episode 58\n",
      "iteration: 1000/1000\tavg score: 9.65e-01\tbest agent: 2\tscore: 2.21e+00 \tdeque: 9.89802e-01\n",
      "Episode 59\n",
      "iteration: 1000/1000\tavg score: 8.98e-01\tbest agent: 13\tscore: 1.87e+00 \tdeque: 9.88254e-01\n",
      "Episode 60\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 6\tscore: 2.28e+00 \tdeque: 9.89342e-01\n",
      "Episode 61\n",
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 5\tscore: 1.78e+00 \tdeque: 9.92123e-01\n",
      "Episode 62\n",
      "iteration: 1000/1000\tavg score: 9.53e-01\tbest agent: 5\tscore: 1.77e+00 \tdeque: 9.91500e-01\n",
      "Episode 63\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 9\tscore: 2.17e+00 \tdeque: 9.92143e-01\n",
      "Episode 64\n",
      "iteration: 1000/1000\tavg score: 8.34e-01\tbest agent: 14\tscore: 1.71e+00 \tdeque: 9.89672e-01\n",
      "Episode 65\n",
      "iteration: 1000/1000\tavg score: 9.12e-01\tbest agent: 18\tscore: 2.83e+00 \tdeque: 9.88477e-01\n",
      "Episode 66\n",
      "iteration: 1000/1000\tavg score: 8.37e-01\tbest agent: 10\tscore: 1.55e+00 \tdeque: 9.86189e-01\n",
      "Episode 67\n",
      "iteration: 1000/1000\tavg score: 9.36e-01\tbest agent: 10\tscore: 2.90e+00 \tdeque: 9.85448e-01\n",
      "Episode 68\n",
      "iteration: 1000/1000\tavg score: 9.84e-01\tbest agent: 19\tscore: 1.71e+00 \tdeque: 9.85434e-01\n",
      "Episode 69\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 4\tscore: 2.12e+00 \tdeque: 9.86841e-01\n",
      "Episode 70\n",
      "iteration: 1000/1000\tavg score: 9.58e-01\tbest agent: 3\tscore: 2.15e+00 \tdeque: 9.86429e-01\n",
      "Episode 71\n",
      "iteration: 1000/1000\tavg score: 7.94e-01\tbest agent: 19\tscore: 1.99e+00 \tdeque: 9.83725e-01\n",
      "Episode 72\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 5\tscore: 2.25e+00 \tdeque: 9.84271e-01\n",
      "Episode 73\n",
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 5\tscore: 2.64e+00 \tdeque: 9.86637e-01\n",
      "Episode 74\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 9\tscore: 2.22e+00 \tdeque: 9.87777e-01\n",
      "Episode 75\n",
      "iteration: 1000/1000\tavg score: 8.55e-01\tbest agent: 8\tscore: 1.61e+00 \tdeque: 9.86007e-01\n",
      "Episode 76\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 5\tscore: 1.69e+00 \tdeque: 9.86309e-01\n",
      "Episode 77\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 15\tscore: 2.00e+00 \tdeque: 9.87182e-01\n",
      "Episode 78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000/1000\tavg score: 8.90e-01\tbest agent: 11\tscore: 2.21e+00 \tdeque: 9.85942e-01\n",
      "Episode 79\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 10\tscore: 2.05e+00 \tdeque: 9.86854e-01\n",
      "Episode 80\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 7\tscore: 2.01e+00 \tdeque: 9.87312e-01\n",
      "Episode 81\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 2\tscore: 2.13e+00 \tdeque: 9.88241e-01\n",
      "Episode 82\n",
      "iteration: 1000/1000\tavg score: 1.00e+00\tbest agent: 9\tscore: 1.96e+00 \tdeque: 9.88409e-01\n",
      "Episode 83\n",
      "iteration: 1000/1000\tavg score: 9.71e-01\tbest agent: 18\tscore: 1.72e+00 \tdeque: 9.88199e-01\n",
      "Episode 84\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 5\tscore: 1.86e+00 \tdeque: 9.89268e-01\n",
      "Episode 85\n",
      "iteration: 1000/1000\tavg score: 8.97e-01\tbest agent: 0\tscore: 1.61e+00 \tdeque: 9.88182e-01\n",
      "Episode 86\n",
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 15\tscore: 2.73e+00 \tdeque: 9.90227e-01\n",
      "Episode 87\n",
      "iteration: 1000/1000\tavg score: 9.16e-01\tbest agent: 10\tscore: 1.94e+00 \tdeque: 9.89379e-01\n",
      "Episode 88\n",
      "iteration: 1000/1000\tavg score: 1.30e+00\tbest agent: 19\tscore: 3.23e+00 \tdeque: 9.92909e-01\n",
      "Episode 89\n",
      "iteration: 1000/1000\tavg score: 9.30e-01\tbest agent: 6\tscore: 2.54e+00 \tdeque: 9.92208e-01\n",
      "Episode 90\n",
      "iteration: 1000/1000\tavg score: 1.33e+00\tbest agent: 19\tscore: 2.21e+00 \tdeque: 9.95950e-01\n",
      "Episode 91\n",
      "iteration: 1000/1000\tavg score: 1.15e+00\tbest agent: 6\tscore: 2.13e+00 \tdeque: 9.97687e-01\n",
      "Episode 92\n",
      "iteration: 1000/1000\tavg score: 9.71e-01\tbest agent: 12\tscore: 2.01e+00 \tdeque: 9.97397e-01\n",
      "Episode 93\n",
      "iteration: 1000/1000\tavg score: 9.87e-01\tbest agent: 16\tscore: 1.70e+00 \tdeque: 9.97285e-01\n",
      "Episode 94\n",
      "iteration: 1000/1000\tavg score: 8.75e-01\tbest agent: 12\tscore: 2.18e+00 \tdeque: 9.95984e-01\n",
      "Episode 95\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 12\tscore: 1.94e+00 \tdeque: 9.96311e-01\n",
      "Episode 96\n",
      "iteration: 1000/1000\tavg score: 1.11e+00\tbest agent: 15\tscore: 2.69e+00 \tdeque: 9.97479e-01\n",
      "Episode 97\n",
      "iteration: 1000/1000\tavg score: 9.85e-01\tbest agent: 13\tscore: 1.71e+00 \tdeque: 9.97350e-01\n",
      "Episode 98\n",
      "iteration: 1000/1000\tavg score: 9.66e-01\tbest agent: 3\tscore: 1.89e+00 \tdeque: 9.97036e-01\n",
      "Episode 99\n",
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 7\tscore: 2.25e+00 \tdeque: 9.98692e-01\n",
      "Episode 100\n",
      "Episode 100\tAverage Score: 9.97975e-0101\tbest agent: 5\tscore: 2.74e+00 \tdeque: 9.97975e-01\n",
      "\n",
      "Episode 101\n",
      "iteration: 1000/1000\tavg score: 8.14e-01\tbest agent: 19\tscore: 1.89e+00 \tdeque: 9.96340e-01\n",
      "Episode 102\n",
      "iteration: 1000/1000\tavg score: 9.84e-01\tbest agent: 10\tscore: 2.85e+00 \tdeque: 9.97810e-01\n",
      "Episode 103\n",
      "iteration: 1000/1000\tavg score: 1.20e+00\tbest agent: 16\tscore: 3.43e+00 \tdeque: 9.98435e-01\n",
      "Episode 104\n",
      "iteration: 1000/1000\tavg score: 7.96e-01\tbest agent: 4\tscore: 1.64e+00 \tdeque: 9.95200e-01\n",
      "Episode 105\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 12\tscore: 2.17e+00 \tdeque: 9.97015e-01\n",
      "Episode 106\n",
      "iteration: 1000/1000\tavg score: 8.46e-01\tbest agent: 11\tscore: 1.50e+00 \tdeque: 9.96020e-01\n",
      "Episode 107\n",
      "iteration: 1000/1000\tavg score: 7.38e-01\tbest agent: 1\tscore: 2.05e+00 \tdeque: 9.93355e-01\n",
      "Episode 108\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 18\tscore: 2.55e+00 \tdeque: 9.96360e-01\n",
      "Episode 109\n",
      "iteration: 1000/1000\tavg score: 8.79e-01\tbest agent: 17\tscore: 1.90e+00 \tdeque: 9.94320e-01\n",
      "Episode 110\n",
      "iteration: 1000/1000\tavg score: 9.64e-01\tbest agent: 12\tscore: 2.51e+00 \tdeque: 9.94085e-01\n",
      "Episode 111\n",
      "iteration: 1000/1000\tavg score: 1.00e+00\tbest agent: 16\tscore: 2.98e+00 \tdeque: 9.92845e-01\n",
      "Episode 112\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 11\tscore: 2.18e+00 \tdeque: 9.95945e-01\n",
      "Episode 113\n",
      "iteration: 1000/1000\tavg score: 8.22e-01\tbest agent: 5\tscore: 1.54e+00 \tdeque: 9.93520e-01\n",
      "Episode 114\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 16\tscore: 2.20e+00 \tdeque: 9.94190e-01\n",
      "Episode 115\n",
      "iteration: 1000/1000\tavg score: 9.60e-01\tbest agent: 10\tscore: 1.99e+00 \tdeque: 9.95720e-01\n",
      "Episode 116\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 7\tscore: 1.93e+00 \tdeque: 9.96495e-01\n",
      "Episode 117\n",
      "iteration: 1000/1000\tavg score: 8.50e-01\tbest agent: 12\tscore: 2.50e+00 \tdeque: 9.96810e-01\n",
      "Episode 118\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 19\tscore: 2.80e+00 \tdeque: 9.96010e-01\n",
      "Episode 119\n",
      "iteration: 1000/1000\tavg score: 9.53e-01\tbest agent: 10\tscore: 3.51e+00 \tdeque: 9.95830e-01\n",
      "Episode 120\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 9\tscore: 2.24e+00 \tdeque: 9.94700e-01\n",
      "Episode 121\n",
      "iteration: 1000/1000\tavg score: 9.11e-01\tbest agent: 7\tscore: 1.66e+00 \tdeque: 9.90835e-01\n",
      "Episode 122\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 13\tscore: 1.95e+00 \tdeque: 9.93175e-01\n",
      "Episode 123\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 1\tscore: 1.90e+00 \tdeque: 9.94360e-01\n",
      "Episode 124\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 18\tscore: 2.54e+00 \tdeque: 9.96140e-01\n",
      "Episode 125\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 14\tscore: 2.29e+00 \tdeque: 9.98625e-01\n",
      "Episode 126\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 8\tscore: 1.90e+00 \tdeque: 9.99480e-01\n",
      "Episode 127\n",
      "iteration: 1000/1000\tavg score: 1.15e+00\tbest agent: 3\tscore: 2.13e+00 \tdeque: 9.99650e-01\n",
      "Episode 128\n",
      "iteration: 1000/1000\tavg score: 9.25e-01\tbest agent: 17\tscore: 2.52e+00 \tdeque: 9.99835e-01\n",
      "Episode 129\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 6\tscore: 1.79e+00 \tdeque: 1.00144e+00\n",
      "Episode 130\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 0\tscore: 1.85e+00 \tdeque: 1.00347e+00\n",
      "Episode 131\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 14\tscore: 2.59e+00 \tdeque: 1.00491e+00\n",
      "Episode 132\n",
      "iteration: 1000/1000\tavg score: 9.50e-01\tbest agent: 10\tscore: 1.87e+00 \tdeque: 1.00409e+00\n",
      "Episode 133\n",
      "iteration: 1000/1000\tavg score: 9.12e-01\tbest agent: 12\tscore: 1.76e+00 \tdeque: 1.00397e+00\n",
      "Episode 134\n",
      "iteration: 1000/1000\tavg score: 9.62e-01\tbest agent: 9\tscore: 2.43e+00 \tdeque: 1.00394e+00\n",
      "Episode 135\n",
      "iteration: 1000/1000\tavg score: 9.33e-01\tbest agent: 14\tscore: 1.67e+00 \tdeque: 1.00453e+00\n",
      "Episode 136\n",
      "iteration: 1000/1000\tavg score: 8.90e-01\tbest agent: 11\tscore: 2.26e+00 \tdeque: 1.00198e+00\n",
      "Episode 137\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 9\tscore: 2.39e+00 \tdeque: 1.00399e+00\n",
      "Episode 138\n",
      "iteration: 1000/1000\tavg score: 1.12e+00\tbest agent: 15\tscore: 1.97e+00 \tdeque: 1.00459e+00\n",
      "Episode 139\n",
      "iteration: 1000/1000\tavg score: 1.00e+00\tbest agent: 12\tscore: 2.41e+00 \tdeque: 1.00616e+00\n",
      "Episode 140\n",
      "iteration: 1000/1000\tavg score: 1.27e+00\tbest agent: 17\tscore: 2.71e+00 \tdeque: 1.00861e+00\n",
      "Episode 141\n",
      "iteration: 1000/1000\tavg score: 9.84e-01\tbest agent: 17\tscore: 1.94e+00 \tdeque: 1.00847e+00\n",
      "Episode 142\n",
      "iteration: 1000/1000\tavg score: 9.03e-01\tbest agent: 19\tscore: 1.87e+00 \tdeque: 1.00771e+00\n",
      "Episode 143\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 10\tscore: 2.12e+00 \tdeque: 1.00756e+00\n",
      "Episode 144\n",
      "iteration: 1000/1000\tavg score: 8.42e-01\tbest agent: 19\tscore: 2.02e+00 \tdeque: 1.00621e+00\n",
      "Episode 145\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 6\tscore: 3.68e+00 \tdeque: 1.00469e+00\n",
      "Episode 146\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 1\tscore: 2.16e+00 \tdeque: 1.00528e+00\n",
      "Episode 147\n",
      "iteration: 1000/1000\tavg score: 9.60e-01\tbest agent: 8\tscore: 1.86e+00 \tdeque: 1.00550e+00\n",
      "Episode 148\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 7\tscore: 1.97e+00 \tdeque: 1.00344e+00\n",
      "Episode 149\n",
      "iteration: 1000/1000\tavg score: 8.44e-01\tbest agent: 4\tscore: 1.96e+00 \tdeque: 1.00274e+00\n",
      "Episode 150\n",
      "iteration: 1000/1000\tavg score: 9.69e-01\tbest agent: 17\tscore: 2.45e+00 \tdeque: 1.00347e+00\n",
      "Episode 151\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 0\tscore: 2.44e+00 \tdeque: 1.00379e+00\n",
      "Episode 152\n",
      "iteration: 1000/1000\tavg score: 7.32e-01\tbest agent: 12\tscore: 2.06e+00 \tdeque: 9.99795e-01\n",
      "Episode 153\n",
      "iteration: 1000/1000\tavg score: 9.04e-01\tbest agent: 16\tscore: 2.26e+00 \tdeque: 9.97650e-01\n",
      "Episode 154\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 19\tscore: 2.98e+00 \tdeque: 9.99035e-01\n",
      "Episode 155\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 12\tscore: 2.58e+00 \tdeque: 1.00216e+00\n",
      "Episode 156\n",
      "iteration: 1000/1000\tavg score: 9.62e-01\tbest agent: 19\tscore: 1.98e+00 \tdeque: 1.00294e+00\n",
      "Episode 157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 10\tscore: 2.13e+00 \tdeque: 1.00368e+00\n",
      "Episode 158\n",
      "iteration: 1000/1000\tavg score: 8.14e-01\tbest agent: 0\tscore: 2.04e+00 \tdeque: 1.00217e+00\n",
      "Episode 159\n",
      "iteration: 1000/1000\tavg score: 9.67e-01\tbest agent: 1\tscore: 1.97e+00 \tdeque: 1.00285e+00\n",
      "Episode 160\n",
      "iteration: 1000/1000\tavg score: 9.91e-01\tbest agent: 2\tscore: 1.83e+00 \tdeque: 1.00223e+00\n",
      "Episode 161\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 13\tscore: 2.28e+00 \tdeque: 1.00080e+00\n",
      "Episode 162\n",
      "iteration: 1000/1000\tavg score: 9.62e-01\tbest agent: 14\tscore: 1.95e+00 \tdeque: 1.00089e+00\n",
      "Episode 163\n",
      "iteration: 1000/1000\tavg score: 9.10e-01\tbest agent: 15\tscore: 1.83e+00 \tdeque: 9.99675e-01\n",
      "Episode 164\n",
      "iteration: 1000/1000\tavg score: 8.35e-01\tbest agent: 12\tscore: 1.69e+00 \tdeque: 9.99690e-01\n",
      "Episode 165\n",
      "iteration: 1000/1000\tavg score: 9.91e-01\tbest agent: 9\tscore: 1.94e+00 \tdeque: 1.00048e+00\n",
      "Episode 166\n",
      "iteration: 1000/1000\tavg score: 1.26e+00\tbest agent: 16\tscore: 3.32e+00 \tdeque: 1.00468e+00\n",
      "Episode 167\n",
      "iteration: 1000/1000\tavg score: 9.33e-01\tbest agent: 14\tscore: 2.14e+00 \tdeque: 1.00464e+00\n",
      "Episode 168\n",
      "iteration: 1000/1000\tavg score: 8.61e-01\tbest agent: 18\tscore: 2.21e+00 \tdeque: 1.00341e+00\n",
      "Episode 169\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 6\tscore: 1.97e+00 \tdeque: 1.00401e+00\n",
      "Episode 170\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 6\tscore: 2.56e+00 \tdeque: 1.00539e+00\n",
      "Episode 171\n",
      "iteration: 1000/1000\tavg score: 9.58e-01\tbest agent: 9\tscore: 2.17e+00 \tdeque: 1.00703e+00\n",
      "Episode 172\n",
      "iteration: 1000/1000\tavg score: 9.18e-01\tbest agent: 10\tscore: 2.88e+00 \tdeque: 1.00598e+00\n",
      "Episode 173\n",
      "iteration: 1000/1000\tavg score: 1.04e+00\tbest agent: 4\tscore: 2.83e+00 \tdeque: 1.00481e+00\n",
      "Episode 174\n",
      "iteration: 1000/1000\tavg score: 1.27e+00\tbest agent: 8\tscore: 2.27e+00 \tdeque: 1.00685e+00\n",
      "Episode 175\n",
      "iteration: 1000/1000\tavg score: 9.89e-01\tbest agent: 2\tscore: 3.10e+00 \tdeque: 1.00819e+00\n",
      "Episode 176\n",
      "iteration: 1000/1000\tavg score: 9.92e-01\tbest agent: 8\tscore: 2.38e+00 \tdeque: 1.00802e+00\n",
      "Episode 177\n",
      "iteration: 1000/1000\tavg score: 9.46e-01\tbest agent: 13\tscore: 1.98e+00 \tdeque: 1.00695e+00\n",
      "Episode 178\n",
      "iteration: 1000/1000\tavg score: 9.26e-01\tbest agent: 6\tscore: 1.45e+00 \tdeque: 1.00731e+00\n",
      "Episode 179\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 10\tscore: 2.54e+00 \tdeque: 1.00758e+00\n",
      "Episode 180\n",
      "iteration: 1000/1000\tavg score: 9.43e-01\tbest agent: 13\tscore: 2.03e+00 \tdeque: 1.00677e+00\n",
      "Episode 181\n",
      "iteration: 1000/1000\tavg score: 8.89e-01\tbest agent: 14\tscore: 2.23e+00 \tdeque: 1.00504e+00\n",
      "Episode 182\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 3\tscore: 2.24e+00 \tdeque: 1.00601e+00\n",
      "Episode 183\n",
      "iteration: 1000/1000\tavg score: 8.94e-01\tbest agent: 9\tscore: 1.94e+00 \tdeque: 1.00524e+00\n",
      "Episode 184\n",
      "iteration: 1000/1000\tavg score: 8.78e-01\tbest agent: 3\tscore: 2.36e+00 \tdeque: 1.00325e+00\n",
      "Episode 185\n",
      "iteration: 1000/1000\tavg score: 8.71e-01\tbest agent: 16\tscore: 1.57e+00 \tdeque: 1.00299e+00\n",
      "Episode 186\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 8\tscore: 2.40e+00 \tdeque: 1.00239e+00\n",
      "Episode 187\n",
      "iteration: 1000/1000\tavg score: 9.94e-01\tbest agent: 16\tscore: 3.13e+00 \tdeque: 1.00317e+00\n",
      "Episode 188\n",
      "iteration: 1000/1000\tavg score: 8.41e-01\tbest agent: 12\tscore: 1.65e+00 \tdeque: 9.98580e-01\n",
      "Episode 189\n",
      "iteration: 1000/1000\tavg score: 8.40e-01\tbest agent: 15\tscore: 1.82e+00 \tdeque: 9.97680e-01\n",
      "Episode 190\n",
      "iteration: 1000/1000\tavg score: 6.53e-01\tbest agent: 6\tscore: 1.37e+00 \tdeque: 9.90925e-01\n",
      "Episode 191\n",
      "iteration: 1000/1000\tavg score: 9.21e-01\tbest agent: 3\tscore: 1.67e+00 \tdeque: 9.88595e-01\n",
      "Episode 192\n",
      "iteration: 1000/1000\tavg score: 9.54e-01\tbest agent: 16\tscore: 1.94e+00 \tdeque: 9.88425e-01\n",
      "Episode 193\n",
      "iteration: 1000/1000\tavg score: 8.72e-01\tbest agent: 0\tscore: 1.53e+00 \tdeque: 9.87280e-01\n",
      "Episode 194\n",
      "iteration: 1000/1000\tavg score: 7.90e-01\tbest agent: 3\tscore: 2.43e+00 \tdeque: 9.86435e-01\n",
      "Episode 195\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 1\tscore: 2.31e+00 \tdeque: 9.87070e-01\n",
      "Episode 196\n",
      "iteration: 1000/1000\tavg score: 9.98e-01\tbest agent: 11\tscore: 1.97e+00 \tdeque: 9.85965e-01\n",
      "Episode 197\n",
      "iteration: 1000/1000\tavg score: 1.29e+00\tbest agent: 1\tscore: 2.56e+00 \tdeque: 9.89010e-01\n",
      "Episode 198\n",
      "iteration: 1000/1000\tavg score: 7.66e-01\tbest agent: 11\tscore: 1.52e+00 \tdeque: 9.87005e-01\n",
      "Episode 199\n",
      "iteration: 1000/1000\tavg score: 9.07e-01\tbest agent: 13\tscore: 2.50e+00 \tdeque: 9.84465e-01\n",
      "Episode 200\n",
      "Episode 200\tAverage Score: 9.85490e-0100\tbest agent: 16\tscore: 2.57e+00 \tdeque: 9.85490e-01\n",
      "\n",
      "Episode 201\n",
      "iteration: 1000/1000\tavg score: 8.27e-01\tbest agent: 11\tscore: 1.51e+00 \tdeque: 9.85620e-01\n",
      "Episode 202\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 3\tscore: 2.28e+00 \tdeque: 9.87165e-01\n",
      "Episode 203\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 3\tscore: 2.01e+00 \tdeque: 9.86115e-01\n",
      "Episode 204\n",
      "iteration: 1000/1000\tavg score: 8.68e-01\tbest agent: 13\tscore: 1.68e+00 \tdeque: 9.86840e-01\n",
      "Episode 205\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 9\tscore: 1.74e+00 \tdeque: 9.86590e-01\n",
      "Episode 206\n",
      "iteration: 1000/1000\tavg score: 9.93e-01\tbest agent: 4\tscore: 2.61e+00 \tdeque: 9.88055e-01\n",
      "Episode 207\n",
      "iteration: 1000/1000\tavg score: 8.32e-01\tbest agent: 18\tscore: 1.62e+00 \tdeque: 9.88995e-01\n",
      "Episode 208\n",
      "iteration: 1000/1000\tavg score: 9.38e-01\tbest agent: 10\tscore: 1.88e+00 \tdeque: 9.87375e-01\n",
      "Episode 209\n",
      "iteration: 1000/1000\tavg score: 9.35e-01\tbest agent: 16\tscore: 2.00e+00 \tdeque: 9.87935e-01\n",
      "Episode 210\n",
      "iteration: 1000/1000\tavg score: 9.48e-01\tbest agent: 16\tscore: 2.49e+00 \tdeque: 9.87780e-01\n",
      "Episode 211\n",
      "iteration: 1000/1000\tavg score: 9.71e-01\tbest agent: 16\tscore: 2.10e+00 \tdeque: 9.87445e-01\n",
      "Episode 212\n",
      "iteration: 1000/1000\tavg score: 1.25e+00\tbest agent: 16\tscore: 2.35e+00 \tdeque: 9.88700e-01\n",
      "Episode 213\n",
      "iteration: 1000/1000\tavg score: 9.82e-01\tbest agent: 17\tscore: 3.08e+00 \tdeque: 9.90300e-01\n",
      "Episode 214\n",
      "iteration: 1000/1000\tavg score: 9.33e-01\tbest agent: 1\tscore: 2.50e+00 \tdeque: 9.88220e-01\n",
      "Episode 215\n",
      "iteration: 1000/1000\tavg score: 9.63e-01\tbest agent: 19\tscore: 1.98e+00 \tdeque: 9.88255e-01\n",
      "Episode 216\n",
      "iteration: 1000/1000\tavg score: 9.53e-01\tbest agent: 6\tscore: 1.52e+00 \tdeque: 9.86760e-01\n",
      "Episode 217\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 12\tscore: 2.68e+00 \tdeque: 9.89095e-01\n",
      "Episode 218\n",
      "iteration: 1000/1000\tavg score: 1.21e+00\tbest agent: 11\tscore: 2.01e+00 \tdeque: 9.90475e-01\n",
      "Episode 219\n",
      "iteration: 1000/1000\tavg score: 8.82e-01\tbest agent: 14\tscore: 1.80e+00 \tdeque: 9.89765e-01\n",
      "Episode 220\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 9\tscore: 2.54e+00 \tdeque: 9.89885e-01\n",
      "Episode 221\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 0\tscore: 2.40e+00 \tdeque: 9.91050e-01\n",
      "Episode 222\n",
      "iteration: 1000/1000\tavg score: 9.39e-01\tbest agent: 6\tscore: 1.75e+00 \tdeque: 9.89515e-01\n",
      "Episode 223\n",
      "iteration: 1000/1000\tavg score: 1.19e+00\tbest agent: 14\tscore: 2.89e+00 \tdeque: 9.90010e-01\n",
      "Episode 224\n",
      "iteration: 1000/1000\tavg score: 1.00e+00\tbest agent: 4\tscore: 1.81e+00 \tdeque: 9.88760e-01\n",
      "Episode 225\n",
      "iteration: 1000/1000\tavg score: 8.29e-01\tbest agent: 7\tscore: 1.56e+00 \tdeque: 9.86450e-01\n",
      "Episode 226\n",
      "iteration: 1000/1000\tavg score: 9.46e-01\tbest agent: 11\tscore: 1.51e+00 \tdeque: 9.85180e-01\n",
      "Episode 227\n",
      "iteration: 1000/1000\tavg score: 8.54e-01\tbest agent: 13\tscore: 2.38e+00 \tdeque: 9.82225e-01\n",
      "Episode 228\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 17\tscore: 2.25e+00 \tdeque: 9.83465e-01\n",
      "Episode 229\n",
      "iteration: 1000/1000\tavg score: 9.96e-01\tbest agent: 5\tscore: 1.87e+00 \tdeque: 9.82645e-01\n",
      "Episode 230\n",
      "iteration: 1000/1000\tavg score: 8.42e-01\tbest agent: 6\tscore: 1.63e+00 \tdeque: 9.80395e-01\n",
      "Episode 231\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 11\tscore: 2.31e+00 \tdeque: 9.79345e-01\n",
      "Episode 232\n",
      "iteration: 1000/1000\tavg score: 8.70e-01\tbest agent: 16\tscore: 1.69e+00 \tdeque: 9.78540e-01\n",
      "Episode 233\n",
      "iteration: 1000/1000\tavg score: 9.66e-01\tbest agent: 13\tscore: 2.00e+00 \tdeque: 9.79085e-01\n",
      "Episode 234\n",
      "iteration: 1000/1000\tavg score: 1.00e+00\tbest agent: 2\tscore: 1.93e+00 \tdeque: 9.79480e-01\n",
      "Episode 235\n",
      "iteration: 1000/1000\tavg score: 8.74e-01\tbest agent: 3\tscore: 1.74e+00 \tdeque: 9.78885e-01\n",
      "Episode 236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 18\tscore: 1.92e+00 \tdeque: 9.80245e-01\n",
      "Episode 237\n",
      "iteration: 1000/1000\tavg score: 8.87e-01\tbest agent: 5\tscore: 1.77e+00 \tdeque: 9.78230e-01\n",
      "Episode 238\n",
      "iteration: 1000/1000\tavg score: 8.22e-01\tbest agent: 12\tscore: 2.31e+00 \tdeque: 9.75275e-01\n",
      "Episode 239\n",
      "iteration: 1000/1000\tavg score: 9.44e-01\tbest agent: 0\tscore: 2.18e+00 \tdeque: 9.74670e-01\n",
      "Episode 240\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 10\tscore: 3.27e+00 \tdeque: 9.72725e-01\n",
      "Episode 241\n",
      "iteration: 1000/1000\tavg score: 9.29e-01\tbest agent: 5\tscore: 1.77e+00 \tdeque: 9.72170e-01\n",
      "Episode 242\n",
      "iteration: 1000/1000\tavg score: 9.71e-01\tbest agent: 1\tscore: 2.24e+00 \tdeque: 9.72855e-01\n",
      "Episode 243\n",
      "iteration: 1000/1000\tavg score: 9.71e-01\tbest agent: 13\tscore: 2.26e+00 \tdeque: 9.72235e-01\n",
      "Episode 244\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 16\tscore: 2.12e+00 \tdeque: 9.74510e-01\n",
      "Episode 245\n",
      "iteration: 1000/1000\tavg score: 8.52e-01\tbest agent: 13\tscore: 2.50e+00 \tdeque: 9.72920e-01\n",
      "Episode 246\n",
      "iteration: 1000/1000\tavg score: 9.39e-01\tbest agent: 7\tscore: 1.84e+00 \tdeque: 9.71985e-01\n",
      "Episode 247\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 11\tscore: 3.39e+00 \tdeque: 9.73310e-01\n",
      "Episode 248\n",
      "iteration: 1000/1000\tavg score: 9.32e-01\tbest agent: 12\tscore: 2.31e+00 \tdeque: 9.71990e-01\n",
      "Episode 249\n",
      "iteration: 1000/1000\tavg score: 9.35e-01\tbest agent: 2\tscore: 2.46e+00 \tdeque: 9.72900e-01\n",
      "Episode 250\n",
      "iteration: 1000/1000\tavg score: 9.48e-01\tbest agent: 13\tscore: 2.10e+00 \tdeque: 9.72685e-01\n",
      "Episode 251\n",
      "iteration: 1000/1000\tavg score: 9.59e-01\tbest agent: 13\tscore: 1.79e+00 \tdeque: 9.71355e-01\n",
      "Episode 252\n",
      "iteration: 1000/1000\tavg score: 1.17e+00\tbest agent: 4\tscore: 2.17e+00 \tdeque: 9.75785e-01\n",
      "Episode 253\n",
      "iteration: 1000/1000\tavg score: 1.28e+00\tbest agent: 8\tscore: 3.10e+00 \tdeque: 9.79585e-01\n",
      "Episode 254\n",
      "iteration: 1000/1000\tavg score: 9.20e-01\tbest agent: 4\tscore: 2.13e+00 \tdeque: 9.77980e-01\n",
      "Episode 255\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 4\tscore: 2.81e+00 \tdeque: 9.77505e-01\n",
      "Episode 256\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 19\tscore: 2.14e+00 \tdeque: 9.79165e-01\n",
      "Episode 257\n",
      "iteration: 1000/1000\tavg score: 1.15e+00\tbest agent: 16\tscore: 2.30e+00 \tdeque: 9.79065e-01\n",
      "Episode 258\n",
      "iteration: 1000/1000\tavg score: 1.04e+00\tbest agent: 13\tscore: 2.42e+00 \tdeque: 9.81330e-01\n",
      "Episode 259\n",
      "iteration: 1000/1000\tavg score: 1.18e+00\tbest agent: 7\tscore: 2.29e+00 \tdeque: 9.83490e-01\n",
      "Episode 260\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 1\tscore: 1.76e+00 \tdeque: 9.83780e-01\n",
      "Episode 261\n",
      "iteration: 1000/1000\tavg score: 9.08e-01\tbest agent: 0\tscore: 1.95e+00 \tdeque: 9.82700e-01\n",
      "Episode 262\n",
      "iteration: 1000/1000\tavg score: 8.26e-01\tbest agent: 18\tscore: 1.24e+00 \tdeque: 9.81345e-01\n",
      "Episode 263\n",
      "iteration: 1000/1000\tavg score: 9.90e-01\tbest agent: 18\tscore: 2.50e+00 \tdeque: 9.82145e-01\n",
      "Episode 264\n",
      "iteration: 1000/1000\tavg score: 1.00e+00\tbest agent: 18\tscore: 1.93e+00 \tdeque: 9.83815e-01\n",
      "Episode 265\n",
      "iteration: 1000/1000\tavg score: 9.59e-01\tbest agent: 12\tscore: 1.80e+00 \tdeque: 9.83495e-01\n",
      "Episode 266\n",
      "iteration: 1000/1000\tavg score: 9.13e-01\tbest agent: 4\tscore: 2.18e+00 \tdeque: 9.80055e-01\n",
      "Episode 267\n",
      "iteration: 1000/1000\tavg score: 1.04e+00\tbest agent: 12\tscore: 2.29e+00 \tdeque: 9.81115e-01\n",
      "Episode 268\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 18\tscore: 2.86e+00 \tdeque: 9.83200e-01\n",
      "Episode 269\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 7\tscore: 1.62e+00 \tdeque: 9.82395e-01\n",
      "Episode 270\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 3\tscore: 2.55e+00 \tdeque: 9.82245e-01\n",
      "Episode 271\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 4\tscore: 1.61e+00 \tdeque: 9.83310e-01\n",
      "Episode 272\n",
      "iteration: 1000/1000\tavg score: 8.90e-01\tbest agent: 10\tscore: 2.01e+00 \tdeque: 9.83025e-01\n",
      "Episode 273\n",
      "iteration: 1000/1000\tavg score: 1.24e+00\tbest agent: 3\tscore: 2.87e+00 \tdeque: 9.84985e-01\n",
      "Episode 274\n",
      "iteration: 1000/1000\tavg score: 8.96e-01\tbest agent: 6\tscore: 1.78e+00 \tdeque: 9.81195e-01\n",
      "Episode 275\n",
      "iteration: 1000/1000\tavg score: 1.19e+00\tbest agent: 14\tscore: 2.53e+00 \tdeque: 9.83160e-01\n",
      "Episode 276\n",
      "iteration: 1000/1000\tavg score: 9.80e-01\tbest agent: 10\tscore: 2.17e+00 \tdeque: 9.83040e-01\n",
      "Episode 277\n",
      "iteration: 1000/1000\tavg score: 9.36e-01\tbest agent: 3\tscore: 1.61e+00 \tdeque: 9.82935e-01\n",
      "Episode 278\n",
      "iteration: 1000/1000\tavg score: 7.53e-01\tbest agent: 10\tscore: 1.60e+00 \tdeque: 9.81200e-01\n",
      "Episode 279\n",
      "iteration: 1000/1000\tavg score: 8.65e-01\tbest agent: 12\tscore: 1.52e+00 \tdeque: 9.79005e-01\n",
      "Episode 280\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 19\tscore: 1.83e+00 \tdeque: 9.79635e-01\n",
      "Episode 281\n",
      "iteration: 1000/1000\tavg score: 1.16e+00\tbest agent: 9\tscore: 2.15e+00 \tdeque: 9.82345e-01\n",
      "Episode 282\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 1\tscore: 1.87e+00 \tdeque: 9.81550e-01\n",
      "Episode 283\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 3\tscore: 2.60e+00 \tdeque: 9.83870e-01\n",
      "Episode 284\n",
      "iteration: 1000/1000\tavg score: 9.94e-01\tbest agent: 2\tscore: 2.04e+00 \tdeque: 9.85025e-01\n",
      "Episode 285\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 10\tscore: 2.42e+00 \tdeque: 9.87200e-01\n",
      "Episode 286\n",
      "iteration: 1000/1000\tavg score: 9.98e-01\tbest agent: 6\tscore: 2.69e+00 \tdeque: 9.86145e-01\n",
      "Episode 287\n",
      "iteration: 1000/1000\tavg score: 1.14e+00\tbest agent: 15\tscore: 2.23e+00 \tdeque: 9.87590e-01\n",
      "Episode 288\n",
      "iteration: 1000/1000\tavg score: 9.34e-01\tbest agent: 4\tscore: 2.11e+00 \tdeque: 9.88525e-01\n",
      "Episode 289\n",
      "iteration: 1000/1000\tavg score: 9.88e-01\tbest agent: 16\tscore: 2.68e+00 \tdeque: 9.90005e-01\n",
      "Episode 290\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 8\tscore: 2.19e+00 \tdeque: 9.93530e-01\n",
      "Episode 291\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 8\tscore: 1.64e+00 \tdeque: 9.94655e-01\n",
      "Episode 292\n",
      "iteration: 1000/1000\tavg score: 9.15e-01\tbest agent: 0\tscore: 2.45e+00 \tdeque: 9.94270e-01\n",
      "Episode 293\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 18\tscore: 1.90e+00 \tdeque: 9.96145e-01\n",
      "Episode 294\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 19\tscore: 2.82e+00 \tdeque: 9.98805e-01\n",
      "Episode 295\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 5\tscore: 3.53e+00 \tdeque: 9.98075e-01\n",
      "Episode 296\n",
      "iteration: 1000/1000\tavg score: 9.08e-01\tbest agent: 9\tscore: 1.94e+00 \tdeque: 9.97175e-01\n",
      "Episode 297\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 14\tscore: 2.80e+00 \tdeque: 9.94350e-01\n",
      "Episode 298\n",
      "iteration: 1000/1000\tavg score: 1.17e+00\tbest agent: 5\tscore: 3.40e+00 \tdeque: 9.98360e-01\n",
      "Episode 299\n",
      "iteration: 1000/1000\tavg score: 9.57e-01\tbest agent: 10\tscore: 1.90e+00 \tdeque: 9.98865e-01\n",
      "Episode 300\n",
      "Episode 300\tAverage Score: 9.99640e-0100\tbest agent: 8\tscore: 2.15e+00 \tdeque: 9.99640e-01\n",
      "\n",
      "Episode 301\n",
      "iteration: 1000/1000\tavg score: 9.64e-01\tbest agent: 9\tscore: 1.65e+00 \tdeque: 1.00101e+00\n",
      "Episode 302\n",
      "iteration: 1000/1000\tavg score: 1.11e+00\tbest agent: 19\tscore: 1.77e+00 \tdeque: 1.00075e+00\n",
      "Episode 303\n",
      "iteration: 1000/1000\tavg score: 9.38e-01\tbest agent: 13\tscore: 2.28e+00 \tdeque: 9.99200e-01\n",
      "Episode 304\n",
      "iteration: 1000/1000\tavg score: 9.13e-01\tbest agent: 2\tscore: 1.92e+00 \tdeque: 9.99650e-01\n",
      "Episode 305\n",
      "iteration: 1000/1000\tavg score: 1.04e+00\tbest agent: 1\tscore: 2.57e+00 \tdeque: 9.99975e-01\n",
      "Episode 306\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 11\tscore: 2.51e+00 \tdeque: 1.00015e+00\n",
      "Episode 307\n",
      "iteration: 1000/1000\tavg score: 8.35e-01\tbest agent: 4\tscore: 1.42e+00 \tdeque: 1.00018e+00\n",
      "Episode 308\n",
      "iteration: 1000/1000\tavg score: 1.15e+00\tbest agent: 3\tscore: 2.03e+00 \tdeque: 1.00231e+00\n",
      "Episode 309\n",
      "iteration: 1000/1000\tavg score: 9.50e-01\tbest agent: 11\tscore: 1.70e+00 \tdeque: 1.00246e+00\n",
      "Episode 310\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 12\tscore: 2.23e+00 \tdeque: 1.00380e+00\n",
      "Episode 311\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 12\tscore: 2.37e+00 \tdeque: 1.00424e+00\n",
      "Episode 312\n",
      "iteration: 1000/1000\tavg score: 9.88e-01\tbest agent: 15\tscore: 2.77e+00 \tdeque: 1.00161e+00\n",
      "Episode 313\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 5\tscore: 2.73e+00 \tdeque: 1.00247e+00\n",
      "Episode 314\n",
      "iteration: 1000/1000\tavg score: 1.12e+00\tbest agent: 6\tscore: 2.09e+00 \tdeque: 1.00436e+00\n",
      "Episode 315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1000/1000\tavg score: 9.05e-01\tbest agent: 1\tscore: 2.27e+00 \tdeque: 1.00378e+00\n",
      "Episode 316\n",
      "iteration: 1000/1000\tavg score: 9.56e-01\tbest agent: 3\tscore: 1.77e+00 \tdeque: 1.00381e+00\n",
      "Episode 317\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 9\tscore: 2.49e+00 \tdeque: 1.00390e+00\n",
      "Episode 318\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 16\tscore: 2.75e+00 \tdeque: 1.00225e+00\n",
      "Episode 319\n",
      "iteration: 1000/1000\tavg score: 1.11e+00\tbest agent: 13\tscore: 2.36e+00 \tdeque: 1.00457e+00\n",
      "Episode 320\n",
      "iteration: 1000/1000\tavg score: 9.55e-01\tbest agent: 16\tscore: 2.05e+00 \tdeque: 1.00384e+00\n",
      "Episode 321\n",
      "iteration: 1000/1000\tavg score: 1.13e+00\tbest agent: 19\tscore: 3.32e+00 \tdeque: 1.00483e+00\n",
      "Episode 322\n",
      "iteration: 1000/1000\tavg score: 9.03e-01\tbest agent: 3\tscore: 3.02e+00 \tdeque: 1.00446e+00\n",
      "Episode 323\n",
      "iteration: 1000/1000\tavg score: 7.68e-01\tbest agent: 10\tscore: 1.66e+00 \tdeque: 1.00023e+00\n",
      "Episode 324\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 7\tscore: 1.98e+00 \tdeque: 1.00122e+00\n",
      "Episode 325\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 2\tscore: 1.88e+00 \tdeque: 1.00350e+00\n",
      "Episode 326\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 18\tscore: 2.60e+00 \tdeque: 1.00497e+00\n",
      "Episode 327\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 18\tscore: 2.44e+00 \tdeque: 1.00742e+00\n",
      "Episode 328\n",
      "iteration: 1000/1000\tavg score: 1.11e+00\tbest agent: 16\tscore: 2.42e+00 \tdeque: 1.00807e+00\n",
      "Episode 329\n",
      "iteration: 1000/1000\tavg score: 9.08e-01\tbest agent: 6\tscore: 2.91e+00 \tdeque: 1.00719e+00\n",
      "Episode 330\n",
      "iteration: 1000/1000\tavg score: 1.10e+00\tbest agent: 2\tscore: 2.85e+00 \tdeque: 1.00972e+00\n",
      "Episode 331\n",
      "iteration: 1000/1000\tavg score: 8.89e-01\tbest agent: 4\tscore: 1.60e+00 \tdeque: 1.00833e+00\n",
      "Episode 332\n",
      "iteration: 1000/1000\tavg score: 9.97e-01\tbest agent: 15\tscore: 1.95e+00 \tdeque: 1.00960e+00\n",
      "Episode 333\n",
      "iteration: 1000/1000\tavg score: 1.08e+00\tbest agent: 19\tscore: 2.05e+00 \tdeque: 1.01079e+00\n",
      "Episode 334\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 7\tscore: 2.05e+00 \tdeque: 1.01101e+00\n",
      "Episode 335\n",
      "iteration: 1000/1000\tavg score: 8.54e-01\tbest agent: 4\tscore: 1.98e+00 \tdeque: 1.01081e+00\n",
      "Episode 336\n",
      "iteration: 1000/1000\tavg score: 1.02e+00\tbest agent: 14\tscore: 1.85e+00 \tdeque: 1.01079e+00\n",
      "Episode 337\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 14\tscore: 2.00e+00 \tdeque: 1.01202e+00\n",
      "Episode 338\n",
      "iteration: 1000/1000\tavg score: 1.17e+00\tbest agent: 9\tscore: 3.42e+00 \tdeque: 1.01554e+00\n",
      "Episode 339\n",
      "iteration: 1000/1000\tavg score: 8.31e-01\tbest agent: 9\tscore: 2.32e+00 \tdeque: 1.01441e+00\n",
      "Episode 340\n",
      "iteration: 1000/1000\tavg score: 1.07e+00\tbest agent: 14\tscore: 2.87e+00 \tdeque: 1.01440e+00\n",
      "Episode 341\n",
      "iteration: 1000/1000\tavg score: 1.09e+00\tbest agent: 14\tscore: 2.20e+00 \tdeque: 1.01598e+00\n",
      "Episode 342\n",
      "iteration: 1000/1000\tavg score: 7.92e-01\tbest agent: 15\tscore: 1.55e+00 \tdeque: 1.01419e+00\n",
      "Episode 343\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 0\tscore: 2.78e+00 \tdeque: 1.01497e+00\n",
      "Episode 344\n",
      "iteration: 1000/1000\tavg score: 1.03e+00\tbest agent: 17\tscore: 1.55e+00 \tdeque: 1.01460e+00\n",
      "Episode 345\n",
      "iteration: 1000/1000\tavg score: 9.42e-01\tbest agent: 17\tscore: 1.79e+00 \tdeque: 1.01550e+00\n",
      "Episode 346\n",
      "iteration: 1000/1000\tavg score: 7.55e-01\tbest agent: 16\tscore: 1.89e+00 \tdeque: 1.01366e+00\n",
      "Episode 347\n",
      "iteration: 1000/1000\tavg score: 9.13e-01\tbest agent: 4\tscore: 2.19e+00 \tdeque: 1.01187e+00\n",
      "Episode 348\n",
      "iteration: 1000/1000\tavg score: 1.04e+00\tbest agent: 14\tscore: 2.41e+00 \tdeque: 1.01297e+00\n",
      "Episode 349\n",
      "iteration: 1000/1000\tavg score: 8.23e-01\tbest agent: 8\tscore: 1.73e+00 \tdeque: 1.01185e+00\n",
      "Episode 350\n",
      "iteration: 1000/1000\tavg score: 9.61e-01\tbest agent: 7\tscore: 2.09e+00 \tdeque: 1.01198e+00\n",
      "Episode 351\n",
      "iteration: 1000/1000\tavg score: 1.31e+00\tbest agent: 14\tscore: 3.50e+00 \tdeque: 1.01554e+00\n",
      "Episode 352\n",
      "iteration: 1000/1000\tavg score: 1.38e+00\tbest agent: 17\tscore: 2.77e+00 \tdeque: 1.01754e+00\n",
      "Episode 353\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 18\tscore: 2.56e+00 \tdeque: 1.01523e+00\n",
      "Episode 354\n",
      "iteration: 1000/1000\tavg score: 8.19e-01\tbest agent: 5\tscore: 2.25e+00 \tdeque: 1.01422e+00\n",
      "Episode 355\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 8\tscore: 2.82e+00 \tdeque: 1.01335e+00\n",
      "Episode 356\n",
      "iteration: 1000/1000\tavg score: 1.01e+00\tbest agent: 1\tscore: 1.80e+00 \tdeque: 1.01221e+00\n",
      "Episode 357\n",
      "iteration: 1000/1000\tavg score: 8.34e-01\tbest agent: 11\tscore: 1.95e+00 \tdeque: 1.00907e+00\n",
      "Episode 358\n",
      "iteration: 1000/1000\tavg score: 1.05e+00\tbest agent: 6\tscore: 2.81e+00 \tdeque: 1.00915e+00\n",
      "Episode 359\n",
      "iteration: 1000/1000\tavg score: 9.93e-01\tbest agent: 12\tscore: 1.87e+00 \tdeque: 1.00725e+00\n",
      "Episode 360\n",
      "iteration: 1000/1000\tavg score: 8.22e-01\tbest agent: 6\tscore: 1.52e+00 \tdeque: 1.00527e+00\n",
      "Episode 361\n",
      "iteration: 1000/1000\tavg score: 8.09e-01\tbest agent: 12\tscore: 1.59e+00 \tdeque: 1.00428e+00\n",
      "Episode 362\n",
      "iteration: 1000/1000\tavg score: 1.06e+00\tbest agent: 16\tscore: 1.94e+00 \tdeque: 1.00665e+00\n",
      "Episode 363\n",
      "iteration: 157/1000\tavg score: 1.51e-01\tbest agent: 3\tscore: 6.10e-01 "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from ddpg_agent20 import Agent, ReplayBuffer\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "\n",
    "# note: the environment max_t maxes out at 1000\n",
    "def ddpg(n_episodes=300, max_t=1000, print_every=100, load_checkpoints=True, checkpoint=0):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    best_scores = []\n",
    "\n",
    "    print('creating agents')\n",
    "    # instantiate a list of agents using single shared replay buffer\n",
    "    replay_buffer = ReplayBuffer(action_size, int(1e6), 64, 42)\n",
    "    agents = [Agent(state_size, action_size, i+42, replay_buffer) for i in range(num_agents)]\n",
    "    \n",
    "    # restart from checkpointed files\n",
    "    if load_checkpoints:\n",
    "        idx = checkpoint\n",
    "        agents = []\n",
    "        for i in range(num_agents):\n",
    "            agents.append(Agent(state_size, action_size, i, replay_buffer))\n",
    "            agents[i].actor_local.load_state_dict(torch.load('checkpoint_actor{:02d}.pth'.format(idx)))\n",
    "            agents[i].actor_target.load_state_dict(torch.load('checkpoint_actor{:02d}.pth'.format(idx)))\n",
    "            agents[i].critic_local.load_state_dict(torch.load('checkpoint_critic{:02d}.pth'.format(idx)))\n",
    "            agents[i].critic_target.load_state_dict(torch.load('checkpoint_critic{:02d}.pth'.format(idx)))\n",
    "\n",
    "    argmax = lambda lst: lst.index(max(lst))\n",
    "    argmin = lambda lst: lst.index(min(lst))\n",
    "    \n",
    "    bestest_score = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        for i in range(num_agents): agents[i].reset()\n",
    "        score = 0\n",
    "        scores = [0]*num_agents\n",
    "\n",
    "        print('\\nEpisode {}'.format(i_episode))\n",
    "        for t in range(max_t):\n",
    "            actions = [agents[i].act(states[i]) for i in range(num_agents)]\n",
    "            env_info = env.step(actions)[brain_name]       # send the action to the environment\n",
    "            next_states = env_info.vector_observations     # get the next state\n",
    "            rewards = env_info.rewards                     # get the reward\n",
    "            dones = env_info.local_done                    # see if episode has finished\n",
    "            for i in range(num_agents):\n",
    "                agents[i].step(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "                scores[i] += rewards[i]\n",
    "            imax = argmax(scores)\n",
    "            imin = argmax(scores)\n",
    "            best_score = scores[imax]\n",
    "            states = next_states\n",
    "            score += sum(rewards) / len(rewards)\n",
    "            print('\\riteration: {}/{}\\tavg score: {:.2e}\\tbest agent: {}\\tscore: {:.2e} '.format(t+1, max_t, score, imax, best_score), end=\"\")\n",
    "\n",
    "            if np.any(dones): break\n",
    "        \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        best_scores.append(best_score)\n",
    "                \n",
    "        with open('scores.log', 'a') as fp:\n",
    "            fp.write(\"{:d}\\t{:2e}\\t{:2e}\\n\".format(i_episode, score, best_score))\n",
    "\n",
    "        print('\\tdeque: {:.5e}'.format(np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        # checkpoint the best agent\n",
    "        torch.save(agents[imax].actor_local.state_dict(), 'checkpoint_actor{:02d}.pth'.format(0))\n",
    "        torch.save(agents[imax].critic_local.state_dict(), 'checkpoint_critic{:02d}.pth'.format(0))\n",
    "\n",
    "        if best_score > bestest_score:\n",
    "            bestest_score = best_score\n",
    "            print('\\ncheckpointing agent {:d} with score {:.2e}'.format(imax, best_score))\n",
    "            torch.save(agents[imax].actor_local.state_dict(), 'checkpoint_actor{:02d}.pth'.format(99))\n",
    "            torch.save(agents[imax].critic_local.state_dict(), 'checkpoint_critic{:02d}.pth'.format(99))\n",
    "            \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.5e}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores, best_scores\n",
    "\n",
    "scores, best_scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strike>Single agent for all arms </strike>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ddpg_agent import Agent\n",
    "# from collections import deque\n",
    "# import torch\n",
    "\n",
    "\n",
    "# def ddpg(n_episodes=200, max_t=1000, print_every=100):\n",
    "#     scores_deque = deque(maxlen=print_every)\n",
    "#     scores = []\n",
    "\n",
    "#     # instantiate \n",
    "#     agent = Agent(state_size=20*state_size, action_size=20*action_size, random_seed=4)\n",
    "#     print(agent.action_size)\n",
    "#     print(agent.state_size)\n",
    "\n",
    "#     for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "#         env_info = env.reset(train_mode=True)[brain_name]\n",
    "#         states = env_info.vector_observations.flatten()\n",
    "#         print(len(states))\n",
    "#         agent.reset()\n",
    "#         score = 0\n",
    "\n",
    "#         for t in range(max_t):\n",
    "#             actions = agent.act(states)\n",
    "#             env_info = env.step(actions)[brain_name]       # send the action to the environment\n",
    "#             next_states = env_info.vector_observations     # get the next state\n",
    "#             rewards = env_info.rewards                     # get the reward\n",
    "#             dones = env_info.local_done                    # see if episode has finished\n",
    "#             agent.step(states, actions, rewards, next_states, dones)\n",
    "#             states = next_states\n",
    "#             scores += rewards\n",
    "#             if np.any(dones): break\n",
    "        \n",
    "#         scores_deque.append(score)\n",
    "#         scores.append(score)\n",
    "        \n",
    "#         print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "#         for i in range(num_agents): torch.save(agents[i].actor_local.state_dict(), 'checkpoint_actor{:02d}.pth'.format(i))\n",
    "#         for i in range(num_agents): torch.save(agents[i].critic_local.state_dict(), 'checkpoint_critic{:02d}.pth'.format(i))\n",
    "#         if i_episode % print_every == 0:\n",
    "#             print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "#     return scores\n",
    "\n",
    "# scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Watch the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "import time\n",
    "time.sleep(30)\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=0)\n",
    "\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "score = 0\n",
    "while True:\n",
    "    state = env_info.vector_observations[0]\n",
    "    action = agent.act(state)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    score += reward                                # update the score\n",
    "    done = env_info.local_done[0]\n",
    "    if done: break\n",
    "        \n",
    "print(\"Final Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
